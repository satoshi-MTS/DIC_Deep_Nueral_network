{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c30831-d003-4251-b3d7-ba7332626fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1f9fa-6ddc-4a6e-a9c0-76d58d19e128",
   "metadata": {},
   "source": [
    "# 【問題1】全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、すべて全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c92e8343-3cd3-4d12-b7ad-9e9766f49b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.init_W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.init_B(n_nodes2)\n",
    "        \n",
    "        # Adagradで使用する各層の前回までの重み\n",
    "        self.H_before_W = np.zeros_like(self.W)\n",
    "        self.H_before_B = np.zeros_like(self.B)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"       \n",
    "        # 逆伝播で使用するためインスタンス化\n",
    "        self.X = X\n",
    "        \n",
    "        A = X @ self.W + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = dA.sum(axis=0)\n",
    "        self.dW = self.X.T @ dA\n",
    "        dZ = dA @ self.W.T\n",
    "        \n",
    "        # 更新\n",
    "        # ここの書き方がよくわかってない\n",
    "        self = optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719d043-1d33-428e-ac65-d6caa56f1de7",
   "metadata": {},
   "source": [
    "# 【問題2】初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6fce39-c8a4-4f1b-8d6d-88c49fbe4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        \n",
    "    def init_W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * self.sigma\n",
    "        return W\n",
    "\n",
    "\n",
    "    def init_B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9114527-392d-4bcd-9b0c-608a513f6007",
   "metadata": {},
   "source": [
    "# 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "969a7d56-05fd-47a6-bbaa-ea3e56cf6796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update_W(self, layer):\n",
    "        \"\"\"\n",
    "        Wの更新\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        --------------\n",
    "        \n",
    "        return\n",
    "        ---------------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        # 引数のメンバ値を更新する\n",
    "        layer.W -= self.lr * layer.dW \n",
    "        layer.B -= self.lr * layer.dB \n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028f2f2-653c-40f0-9c4d-0500e0a6bcf9",
   "metadata": {},
   "source": [
    "# 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。\n",
    "\n",
    "\n",
    "発展的要素\n",
    "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f416e9d-bbcf-4bd8-9786-44af6ac05159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    sigmoid関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = 1 / (1 + np.exp(A))\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dA = dZ * (self.Z * (1 - self.Z))\n",
    "        \n",
    "        return dA\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bae2f8ba-4088-4149-bae1-551f8ccd10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    softmax関数\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        \"\"\"\n",
    "        Z = np.exp(A) / np.exp(A).sum(axis=1).reshape(-1, 1)\n",
    "        \n",
    "        # 逆伝播でつかうためインスタンス化\n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        # 交差エントロピー誤差層の逆伝播+softmax層の逆伝播\n",
    "        \n",
    "        dA = 1 / y.shape[0] * (self.Z - y)\n",
    "        \n",
    "        return dA\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064eac0-ab6d-4734-a3a4-25b9551146e1",
   "metadata": {},
   "source": [
    "# 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。\n",
    "\n",
    "\n",
    "Xavierの初期値\n",
    "Xavierの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\frac{1}{\\sqrt{n}}$$\n",
    "$n$ : 前の層のノード数\n",
    "\n",
    "Heの初期値\n",
    "Heの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{n}}$$\n",
    "\n",
    "$n$ : 前の層のノード数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ab299f-8945-45e4-a08c-dab6c97ad1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \n",
    "    \n",
    "    def init_W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    \n",
    "    def init_B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67211649-6941-4a81-a894-59310f4b190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "\n",
    "    def init_W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W : 重みの初期値\n",
    "        \"\"\"\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "\n",
    "    def init_B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :　バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb8a81-616d-441c-aa45-7f015d78b17d",
   "metadata": {},
   "source": [
    "# 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "$$W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i})B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})$$\n",
    "\n",
    "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的にはすべて同じとする）\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$E()$ : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "\n",
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 $H$ を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。\n",
    "\n",
    "$$H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i})$$\n",
    "\n",
    "$H_i$ : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "\n",
    "$H_i^{\\prime}$ : 更新した $H_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d0d2a04-438d-498b-ac0a-4555436ee47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    各FC層の重み及びバイアスの更新\n",
    "    \n",
    "    param\n",
    "    -------------\n",
    "    lr : 学習率\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.delta = 1e-7\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Wの更新\n",
    "        \n",
    "        param\n",
    "        --------------\n",
    "        layer : FCクラスのインスタンス\n",
    "        --------------\n",
    "        \n",
    "        return\n",
    "        ---------------\n",
    "        layer : FCクラスのインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        hW = layer.H_before_W + (layer.dW * layer.dW)\n",
    "        # h.shape = dw.shape = w.shape\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW / np.sqrt(hW + self.delta)\n",
    "        # 割り算なのでdW / hは形が変わらないはず(掛け算であればアダマール積を取る。)\n",
    "        # そもそも行列割り算という概念がないため、割り算は同じ形じゃないとできない\n",
    "        \n",
    "        layer.H_before_W = hW\n",
    "        # 次使う場合のためにhは更新する\n",
    "                 \n",
    "        hB = layer.H_before_B + (layer.dB * layer.dB)\n",
    "        \n",
    "        layer.B -= self.lr * layer.dB / np.sqrt(hB + self.delta)\n",
    "        \n",
    "        layer.H_before_B = hB\n",
    "        # 次使う場合のためにhは更新する\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f2afe-4036-43a6-8eba-0c1abf0a9db1",
   "metadata": {},
   "source": [
    "# データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "601aa200-2f8e-433f-ad3b-931be97ef1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 画像データ→行データ\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# one-hotベクトル化\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "\n",
    "# one-hotのデータ分割\n",
    "X_train, X_val, y_train_one_hot, y_val_one_hot = train_test_split(X_train, y_train_one_hot, stratify=y_train_one_hot, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0e517-508c-4a34-aecb-2fa1851c0caa",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0ff54d3-df0d-4be6-a330-1700301d209b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdaGrad(lr=0.1)\n",
    "test = Affine(n_nodes1=784, n_nodes2=50, initializer=SimpleInitializer(0.01), optimizer=optimizer)\n",
    "dA = test.forward(X_train)\n",
    "test.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97739338",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1, -0.1, -0.1, -0.1,  0.1,  0.1, -0.1, -0.1, -0.1, -0.1,  0.1,\n",
       "        0.1, -0.1, -0.1,  0.1,  0.1,  0.1,  0.1,  0.1, -0.1, -0.1,  0.1,\n",
       "        0.1,  0.1,  0.1, -0.1,  0.1, -0.1, -0.1,  0.1,  0.1, -0.1, -0.1,\n",
       "        0.1,  0.1, -0.1, -0.1, -0.1,  0.1, -0.1, -0.1, -0.1,  0.1, -0.1,\n",
       "       -0.1, -0.1,  0.1, -0.1, -0.1, -0.1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.backward(dA)\n",
    "test.B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5e5c5-faae-4d70-a279-6ed4a603a0bf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# 【問題8】クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f34450f4-ae9b-4441-89d0-6ceb5ef6a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        # ceilは切り上げ関数\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
    "    \n",
    "    # len関数が使われるとこの値を返す。\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    # 要素をインスタンス変数に入れると値を返す\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    \n",
    "    # iterが使われるとこの値を返す、なんでselfそのものを返す？？\n",
    "    # そういう物っぽい、nextと組み合わせて使われる\n",
    "    # for分繰り返されるという訳ではなく、forを行う前に一度だけ実行される\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    # forの回数実行される\n",
    "    def __next__(self):\n",
    "        # 要素の終わりまでいったら自動的に終了する\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1faac44d-e1c0-448d-abfa-8dd9e206475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, n_output, epoch=10, lr=0.01, sigma=0.1, verbose = True):\n",
    "        \"\"\"\n",
    "        self.sigma : ガウス分布の標準偏差\n",
    "        self.lr : 学習率\n",
    "        self.n_nodes1 : 1層目のノード数\n",
    "        self.n_nodes2 : 2層目のノード数\n",
    "        self.n_output : 出力層のノード数\n",
    "        self.epoch : エポック数\n",
    "        self.loss_train : 訓練データの損失\n",
    "        self.loss_val : 検証データの損失\n",
    "        self.verbose : 学習過程を表示するか\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output\n",
    "        self.epoch = epoch\n",
    "        self.loss_train = np.zeros(epoch)\n",
    "        self.loss_val = np.zeros(epoch)\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        \n",
    "    def get_loss(self, X, y_ture):\n",
    "        \"\"\"\n",
    "        クロスエントロピー誤差を計算\n",
    "        log(X)が最大値0の値を取るため-をかける必要がある。\n",
    "        全て正解の場合0を取る。\n",
    "\n",
    "        param\n",
    "        -------------------\n",
    "        X : 次の形のndarray(batch_size, n_features)\n",
    "        入力値\n",
    "        y_ture : 次の形のndarray(batch_size, n_class)\n",
    "        正解ラベル\n",
    "\n",
    "        return\n",
    "        --------------------\n",
    "        L : float(スカラー)\n",
    "        \"\"\"\n",
    "\n",
    "        h = 1e-7      \n",
    "        L = - np.sum(y_ture * np.log(X + h) / len(y_ture))\n",
    "        return L\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "        訓練データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1層目の入力特徴量数\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # Affine, optimizer, initializerの決定\n",
    "        # Affine :3層, optimizer : SGD, initializer : simple\n",
    "        optimizer = SGD(self.lr)\n",
    "        self.Affine1 = Affine(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = Sigmoid()\n",
    "        self.Affine2 = Affine(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = Sigmoid()\n",
    "        self.Affine3 = Affine(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Softmax()      \n",
    "        \n",
    "        # エポック毎に更新\n",
    "        for i in range(self.epoch):\n",
    "            \n",
    "            # バッチ処理を実行\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            \n",
    "            #\n",
    "            # バッチでループ\n",
    "            #\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                # 順伝播\n",
    "                A1 = self.Affine1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.Affine2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                # 逆伝播\n",
    "                dA3 = self.activation3.backward(mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.Affine3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.Affine2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.Affine1.backward(dA1) # dZ0は使用しない\n",
    "                \n",
    "            #\n",
    "            # 更新パラメータを使って全データで検証\n",
    "            #\n",
    "            A1 = self.Affine1.forward(X)\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.Affine2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.Affine3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "            # lossの計算\n",
    "            self.loss_train[i] = self.get_loss(Z3, y)\n",
    "            \n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            if self.verbose:\n",
    "                print('loss : {}'.format(self.loss_train[i]))\n",
    "        \n",
    "            # 検証データ\n",
    "            if X_val is not None:\n",
    "                #\n",
    "                # 検証データの出力値を求める\n",
    "                #\n",
    "                A1 = self.Affine1.forward(X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.Affine2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Affine3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                # lossの計算\n",
    "                self.loss_val[i] = self.get_loss(Z3, y_val) \n",
    "            \n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        A1 = self.Affine1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.Affine2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.Affine3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        \n",
    "        # 最も大きいインデックスをクラスとして採用\n",
    "        return np.argmax(LZ3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0337783-5fbe-4b2f-857b-5fc12cadd02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ScratchDeepNeuralNetrowkClassifier(n_nodes1=400, n_nodes2=200, n_output=10, verbose=False)\n",
    "test.fit(X_train[:10000], y_train_one_hot[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a627dc9-e759-4b74-b7a4-d7ae0ad3e9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.3632448 , 2.34786935, 2.33917457, 2.33553228, 2.33280891,\n",
       "       2.33079696, 2.32922318, 2.32794589, 2.32749088, 2.32619874])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240bfad-fc8b-4a86-a1e1-2ae8b6952d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Deep_Nueral_network)",
   "language": "python",
   "name": "pycharm-92a049dd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
